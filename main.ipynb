{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hex_datasets/hex_2x2_10000_games_2_turns_before_win.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m n_games \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     15\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhex_datasets/hex_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mboard_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mboard_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_games\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_games_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_turns_before_win\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_turns_before_win.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[1;32m     19\u001b[0m train_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m     21\u001b[0m train_end_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;241m*\u001b[39mtrain_split)\n",
      "File \u001b[0;32m/home/skage/miniconda3/envs/Hex/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/home/skage/miniconda3/envs/Hex/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/home/skage/miniconda3/envs/Hex/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/home/skage/miniconda3/envs/Hex/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/home/skage/miniconda3/envs/Hex/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hex_datasets/hex_2x2_10000_games_2_turns_before_win.csv'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#parser = argparse.ArgumentParser(description=\"Set board size for Hex dataset.\")\n",
    "#parser.add_argument(\"--board_size\", type=int, default=3, help=\"Size of the Hex board (default: 7)\")\n",
    "\n",
    "#args = parser.parse_args()\n",
    "\n",
    "board_size = 4\n",
    "\n",
    "n_turns_before_win = 2\n",
    "n_games = 10000\n",
    "\n",
    "path = f'hex_datasets/hex_{board_size}x{board_size}_{n_games}_games_{n_turns_before_win}_turns_before_win.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "train_split = 0.8\n",
    "\n",
    "train_end_index = int(len(df)*train_split)\n",
    "\n",
    "print(train_end_index)\n",
    "\n",
    "df_train = df[0:train_end_index]\n",
    "df_test = df[train_end_index:]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "from GraphTsetlinMachine.graphs import Graphs\n",
    "\n",
    "HYPERVECTOR_SIZE = 120\n",
    "HYPERVECTOR_BITS = 2\n",
    "\n",
    "x_positon_symbols = [f'x:{x}' for x in range(board_size + 2)]\n",
    "y_positon_symbols = [f'y:{y}' for y in range(board_size + 2)]\n",
    "\n",
    "graph_node_args = {\n",
    "    'symbols' : ['PlayerX', 'PlayerXEdge', 'Empty','PlayerO', 'PlayerOEdge'] + x_positon_symbols + y_positon_symbols,\n",
    "    'hypervector_size': HYPERVECTOR_SIZE,\n",
    "    'hypervector_bits': HYPERVECTOR_BITS\n",
    "}\n",
    "\n",
    "\n",
    "assert board_size == int(np.sqrt(len(df_train.iloc[0, :-1])))\n",
    "\n",
    "num_nodes = board_size*board_size + (2*(board_size+1)) + (2 * board_size)     #Adds edges as colored nodes\n",
    "\n",
    "print(num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Nodes for the board dimension and add initialize graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0:0', '0:1', '0:2', '0:3', '0:4', '0:5', '0:6', '0:7', '0:8', '1:0', '1:1', '1:2', '1:3', '1:4', '1:5', '1:6', '1:7', '1:8', '2:0', '2:1', '2:2', '2:3', '2:4', '2:5', '2:6', '2:7', '2:8', '3:0', '3:1', '3:2', '3:3', '3:4', '3:5', '3:6', '3:7', '3:8', '4:0', '4:1', '4:2', '4:3', '4:4', '4:5', '4:6', '4:7', '4:8', '5:0', '5:1', '5:2', '5:3', '5:4', '5:5', '5:6', '5:7', '5:8', '6:0', '6:1', '6:2', '6:3', '6:4', '6:5', '6:6', '6:7', '6:8', '7:0', '7:1', '7:2', '7:3', '7:4', '7:5', '7:6', '7:7', '7:8', '8:0', '8:1', '8:2', '8:3', '8:4', '8:5', '8:6', '8:7', '8:8']\n",
      "['0:1', '0:2', '0:3', '0:4', '0:5', '0:6', '0:7', '0:8', '1:0', '1:1', '1:2', '1:3', '1:4', '1:5', '1:6', '1:7', '1:8', '2:0', '2:1', '2:2', '2:3', '2:4', '2:5', '2:6', '2:7', '2:8', '3:0', '3:1', '3:2', '3:3', '3:4', '3:5', '3:6', '3:7', '3:8', '4:0', '4:1', '4:2', '4:3', '4:4', '4:5', '4:6', '4:7', '4:8', '5:0', '5:1', '5:2', '5:3', '5:4', '5:5', '5:6', '5:7', '5:8', '6:0', '6:1', '6:2', '6:3', '6:4', '6:5', '6:6', '6:7', '6:8', '7:0', '7:1', '7:2', '7:3', '7:4', '7:5', '7:6', '7:7', '7:8', '8:0', '8:1', '8:2', '8:3', '8:4', '8:5', '8:6', '8:7']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_node_names = [f'{i}:{j}' for i in range(board_size+2) for j in range(board_size+2)]\n",
    "print(all_node_names)\n",
    "\n",
    "all_node_names = all_node_names[1:-1]     #Get all nodes including edges represented as nodes\n",
    "print(all_node_names)\n",
    "\n",
    "\n",
    "graphs_train = Graphs(\n",
    "    train_end_index,\n",
    "    **graph_node_args\n",
    ")\n",
    "\n",
    "\n",
    "graphs_test = Graphs(\n",
    "    len(df_test),\n",
    "    **graph_node_args,\n",
    "    init_with=graphs_train\n",
    ")\n",
    "\n",
    "#Initalize graphs\n",
    "for graph_id in range(len(df_train)):\n",
    "    graphs_train.set_number_of_graph_nodes(graph_id, number_of_graph_nodes=np.uint32(len(all_node_names)))\n",
    "\n",
    "for graph_id in range(len(df_test)):\n",
    "    graphs_test.set_number_of_graph_nodes(graph_id, number_of_graph_nodes=np.uint32(len(all_node_names)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corner Nodes: ['0:1', '1:0', '7:8', '8:7', '0:8', '8:0']\n",
      "Remaining Edge Nodes: ['0:2', '0:3', '0:4', '0:5', '0:6', '0:7', '1:8', '2:0', '2:8', '3:0', '3:8', '4:0', '4:8', '5:0', '5:8', '6:0', '6:8', '7:0', '8:1', '8:2', '8:3', '8:4', '8:5', '8:6']\n",
      "Number of remaining Board Nodes: 49\n"
     ]
    }
   ],
   "source": [
    "graphs_train.prepare_node_configuration()\n",
    "graphs_test.prepare_node_configuration()\n",
    "from copy import copy\n",
    "\n",
    "# Add corner edge nodes\n",
    "remianing_nodes = copy(all_node_names)\n",
    "corner_nodes = ['0:1', '1:0', f'{board_size}:{board_size+1}', f'{board_size+1}:{board_size}', f'{0}:{board_size+1}', f'{board_size+1}:{0}']\n",
    "print('Corner Nodes:', corner_nodes)\n",
    "for node_name in corner_nodes:\n",
    "    remianing_nodes.remove(node_name)\n",
    "\n",
    "#Add remaining edge nodes\n",
    "remaining_edge_nodes = [node for node in remianing_nodes if ('0' in node.split(':') or f'{board_size+1}' in node.split(':'))]\n",
    "print('Remaining Edge Nodes:', remaining_edge_nodes)\n",
    "for node_name in remaining_edge_nodes:\n",
    "    remianing_nodes.remove(node_name)\n",
    "\n",
    "print('Number of remaining Board Nodes:', len(remianing_nodes))\n",
    "\n",
    "\n",
    "\n",
    "#Initalise nodes in train graph\n",
    "for graph_id in range(len(df_train)):\n",
    "    for node_name in corner_nodes:\n",
    "        graphs_train.add_graph_node(graph_id, node_name=node_name, number_of_graph_node_edges=1)\n",
    "    \n",
    "    for node_name in remaining_edge_nodes:\n",
    "        graphs_train.add_graph_node(graph_id, node_name=node_name, number_of_graph_node_edges=2)\n",
    "    \n",
    "    for node_name in remianing_nodes:\n",
    "        graphs_train.add_graph_node(graph_id, node_name=node_name, number_of_graph_node_edges=6)\n",
    "    \n",
    "\n",
    "for graph_id in range(len(df_test)):\n",
    "    for node_name in corner_nodes:\n",
    "        graphs_test.add_graph_node(graph_id, node_name=node_name, number_of_graph_node_edges=1)\n",
    "    \n",
    "    for node_name in remaining_edge_nodes:\n",
    "        graphs_test.add_graph_node(graph_id, node_name=node_name, number_of_graph_node_edges=2)\n",
    "    \n",
    "    for node_name in remianing_nodes:\n",
    "        graphs_test.add_graph_node(graph_id, node_name=node_name, number_of_graph_node_edges=6)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_train.prepare_edge_configuration()\n",
    "graphs_test.prepare_edge_configuration()\n",
    "\n",
    "directions = {\n",
    "    (1, -1) : 'UpRight',\n",
    "    (0, -1) : 'UpLeft',\n",
    "    (1, 0) : 'Right',\n",
    "    (-1, 0) : 'Left',\n",
    "    (0, 1) : 'DownRight',\n",
    "    (-1, 1) : 'DownLeft'\n",
    "}\n",
    "\n",
    "\n",
    "def add_offset_to_node(node_name, offset):\n",
    "    #print(node_name, offset)\n",
    "    i, j = map(int, node_name.split(':'))\n",
    "    i += offset[0]\n",
    "    j += offset[1]\n",
    "    return ':'.join([str(i), str(j)])\n",
    "\n",
    "all_offsets = directions.keys()\n",
    "    \n",
    "\n",
    "\n",
    "#Add remaining edge nodes\n",
    "edges = []\n",
    "\n",
    "#Add edges for all edge nodes\n",
    "for node_name in all_node_names:\n",
    "    for offset in all_offsets:\n",
    "        neighbor = add_offset_to_node(node_name, offset)\n",
    "        #print(neighbor)\n",
    "        if node_name in corner_nodes or node_name in remaining_edge_nodes:      #For edge nodes, only add game tiles as edges.\n",
    "            if neighbor in remianing_nodes:\n",
    "                edges.append({\n",
    "                    'from': node_name,\n",
    "                    'to': neighbor,\n",
    "                    'edge_type': directions[offset]\n",
    "                })\n",
    "        else:\n",
    "            edges.append({\n",
    "                    'from': node_name,\n",
    "                    'to': neighbor,\n",
    "                    'edge_type': directions[offset]\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for graph_id in range(len(df_train)):\n",
    "    for edge in edges:\n",
    "        graphs_train.add_graph_node_edge(graph_id, edge['from'], edge['to'], edge['edge_type'])\n",
    "\n",
    "for graph_id in range(len(df_test)):\n",
    "    for edge in edges:\n",
    "        graphs_test.add_graph_node_edge(graph_id, edge['from'], edge['to'], edge['edge_type'])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Graph Node Property and Extract winning player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.empty(len(df_train), dtype=np.uint32)\n",
    "\n",
    "for graph_id, row in df_train.iterrows():\n",
    "    for node_name in all_node_names:\n",
    "        if node_name.split(\":\")[1] in ['0', f'{board_size+1}']:\n",
    "            graphs_train.add_graph_node_property(graph_id, node_name=node_name, symbol='PlayerXEdge')\n",
    "        elif node_name.split(\":\")[0] in ['0', f'{board_size+1}']:\n",
    "            graphs_train.add_graph_node_property(graph_id, node_name=node_name, symbol='PlayerOEdge')\n",
    "        else:\n",
    "            assert node_name in remianing_nodes\n",
    "            cell_value = row[f'cell{add_offset_to_node(node_name, (-1, -1)).replace(':', '_')}']\n",
    "            if cell_value == -1:\n",
    "                symbol = 'PlayerO'\n",
    "            elif cell_value == 0:\n",
    "                symbol = 'Empty'\n",
    "            elif cell_value == 1:\n",
    "                symbol = 'PlayerX'\n",
    "\n",
    "            graphs_train.add_graph_node_property(graph_id, node_name=node_name, symbol=symbol)\n",
    "        graphs_train.add_graph_node_property(graph_id, node_name=node_name, symbol=f'x:{node_name.split(':')[0]}')   #add x position symbol\n",
    "        graphs_train.add_graph_node_property(graph_id, node_name=node_name, symbol=f'y:{node_name.split(':')[1]}')   #add y position symbol\n",
    "    # Get winning player\n",
    "    Y_train[graph_id] = row['winner'] if row['winner'] == 1 else 2\n",
    "\n",
    "graphs_train.encode()\n",
    "\n",
    "\n",
    "Y_test = np.empty(len(df_test), dtype=np.uint32)\n",
    "\n",
    "for graph_id, row in df_test.iterrows():\n",
    "    graph_id = graph_id - len(df_train)\n",
    "    for node_name in all_node_names:\n",
    "        if node_name.split(\":\")[1] in ['0', f'{board_size+1}']:\n",
    "            graphs_test.add_graph_node_property(graph_id, node_name=node_name, symbol='PlayerXEdge')\n",
    "        elif node_name.split(\":\")[0] in ['0', f'{board_size+1}']:\n",
    "            graphs_test.add_graph_node_property(graph_id, node_name=node_name, symbol='PlayerOEdge')\n",
    "        else:\n",
    "            assert node_name in remianing_nodes\n",
    "            cell_value = row[f'cell{add_offset_to_node(node_name, (-1, -1)).replace(':', '_')}']\n",
    "            if cell_value == -1:\n",
    "                symbol = 'PlayerO'\n",
    "            elif cell_value == 0:\n",
    "                symbol = 'Empty'\n",
    "            elif cell_value == 1:\n",
    "                symbol = 'PlayerX'\n",
    "            graphs_test.add_graph_node_property(graph_id, node_name=node_name, symbol=symbol)\n",
    "        graphs_test.add_graph_node_property(graph_id, node_name=node_name, symbol=f'x:{node_name.split(':')[0]}')   #add x position symbol\n",
    "        graphs_test.add_graph_node_property(graph_id, node_name=node_name, symbol=f'y:{node_name.split(':')[1]}')   #add y position symbol\n",
    "    # Get winning player\n",
    "    Y_test[graph_id] = row['winner'] if row['winner'] == 1 else 2\n",
    "\n",
    "graphs_test.encode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Logging function\n",
    "import json\n",
    "import os\n",
    "run_number = -1\n",
    "def log_if_best_result(accuracy_eval, **kwargs):\n",
    "    existing_result = False\n",
    "    log = False\n",
    "    \n",
    "        \n",
    "    \n",
    "    if run_number != -1:    #Default value\n",
    "        result_folder = os.path.join('results/', f'{board_size}x{board_size}_board_{n_turns_before_win}_turns_before_win')\n",
    "        file_name = f'run_{run_number}.json'\n",
    "            \n",
    "    else:\n",
    "        result_folder = 'results/'\n",
    "        file_name = f'{board_size}x{board_size}_board_{n_turns_before_win}_turns_before_win.json'\n",
    "        \n",
    "    if not os.path.exists(result_folder):\n",
    "        os.makedirs(result_folder)\n",
    "\n",
    "    path = os.path.join(result_folder, file_name)\n",
    "    \n",
    "    log_object = {'eval_accuracy' : accuracy_eval, 'n_train_games': n_games, **kwargs}\n",
    "    if file_name not in os.listdir(result_folder):\n",
    "        log = True\n",
    "    else:\n",
    "        existing_result = True\n",
    "        with open(path, 'r') as f:\n",
    "            json_object = json.load(f)\n",
    "            best_accuracy = json_object['eval_accuracy']\n",
    "            if accuracy_eval > best_accuracy:\n",
    "                log = True\n",
    "                \n",
    "            elif accuracy_eval == 100:\n",
    "                if log_object['TM_args']['number_of_clauses'] < json_object['TM_args']['number_of_clauses']:\n",
    "                    log = True\n",
    "    if log:\n",
    "        print(f'New best result for board size {board_size}, {n_turns_before_win} turns before win\\nAccuracy : {accuracy_eval} with {log_object['TM_args']['number_of_clauses']} clauses.')\n",
    "        if existing_result:\n",
    "            print(f'Previouis best accuracy was: {best_accuracy} with {json_object['TM_args']['number_of_clauses']} clauses.')\n",
    "        print('Logging paramters')\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(log_object, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of sparse structure.\n",
      "Class distribution: Class 1: 4003, Class 2: 3997\n",
      "Dummy accuracy = 0.500375\n",
      "Epoch: 3, Accuracy_train: 49.96,  Accuracy_test: 50.15, Training time: 99.54, Testing time: 0.58\n",
      "Clause #0 W:(-1 -3) x52 AND x69 AND x77\n",
      "Clause #1 W:(-1 -9) \n",
      "[[ 85  92]\n",
      " [ 76  11]\n",
      " [ 45 108]\n",
      " [ 20   4]\n",
      " [ 11  52]\n",
      " [105  52]\n",
      " [ 12  50]\n",
      " [ 84  56]\n",
      " [ 86  34]\n",
      " [  7  20]\n",
      " [ 31  71]\n",
      " [ 72  22]\n",
      " [ 83  36]\n",
      " [ 69  77]\n",
      " [ 61  57]\n",
      " [ 57  15]\n",
      " [ 86  82]\n",
      " [108  37]\n",
      " [ 85  41]\n",
      " [ 65  45]\n",
      " [ 91  90]\n",
      " [  8 118]\n",
      " [  1  70]]\n",
      "[[ 1  3]\n",
      " [ 0 15]]\n",
      "{'Right': 0, 'UpRight': 1, 'DownRight': 2, 'UpLeft': 3, 'Left': 4, 'DownLeft': 5}\n"
     ]
    }
   ],
   "source": [
    "from GraphTsetlinMachine.tm import MultiClassGraphTsetlinMachine\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "#TM Hyper paremeters            Include defeault values in NoisyXOR\n",
    "TM_args = {\n",
    "    'number_of_clauses' : 2,           #Default    10\n",
    "    'T' : 1,                          #Default   100\n",
    "    's' : 1,                         #Default   1.0\n",
    "    'number_of_state_bits' : 32,        #Default     2\n",
    "    'depth' : 2,                        #Default     8\n",
    "    'message_size' : 16,              #Default   256\n",
    "    'message_bits' : 2,                 #Default     2\n",
    "    'max_included_literals' : 2,       #Default     4\n",
    "    'double_hashing': False             #Default False\n",
    "}\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "tm = MultiClassGraphTsetlinMachine(\n",
    "    **TM_args,\n",
    "    grid = (1, 1, 1),\n",
    "    block = (256, 1, 1)\n",
    ")\n",
    "values, counts = np.unique_counts(Y_train)\n",
    "print(f'Class distribution: Class {values[0]}: {counts[0]}, Class {values[1]}: {counts[1]}')\n",
    "print(f'Dummy accuracy = {max(counts)/sum(counts)}')\n",
    "\n",
    "assert len(graphs_train.number_of_graph_nodes) == len(Y_train)\n",
    "#Train and Eval\n",
    "\n",
    "start_training = time()\n",
    "tm.fit(graphs_train, Y_train, epochs=EPOCHS, incremental=True)\n",
    "stop_training = time()\n",
    "\n",
    "start_testing = time()\n",
    "result_test = 100*(tm.predict(graphs_test) == Y_test).mean()\n",
    "stop_testing = time()\n",
    "\n",
    "result_train = 100*(tm.predict(graphs_train) == Y_train).mean()\n",
    "\n",
    "print(\"Epoch: %d, Accuracy_train: %.2f,  Accuracy_test: %.2f, Training time: %.2f, Testing time: %.2f\" % (3, result_train, result_test, stop_training-start_training, stop_testing-start_testing))\n",
    "\n",
    "#log_if_best_result(result_test, train_accuracy=result_train, EPOCHS=EPOCHS, hypervector_size=HYPERVECTOR_SIZE, hypervector_bits=HYPERVECTOR_BITS, train_time=stop_training-start_training,TM_args=TM_args)\n",
    "\n",
    "weights = tm.get_state()[1].reshape(2, -1)\n",
    "for i in range(tm.number_of_clauses):\n",
    "        print(\"Clause #%d W:(%d %d)\" % (i, weights[0,i], weights[1,i]), end=' ')\n",
    "        l = []\n",
    "        for k in range(HYPERVECTOR_SIZE * 2):\n",
    "            if tm.ta_action(0, i, k):\n",
    "                if k < HYPERVECTOR_SIZE:\n",
    "                    l.append(\"x%d\" % (k))\n",
    "                else:\n",
    "                    l.append(\"NOT x%d\" % (k - HYPERVECTOR_SIZE))\n",
    "\n",
    "        # for k in range(args.message_size * 2):\n",
    "        #     if tm.ta_action(1, i, k):\n",
    "        #         if k < args.message_size:\n",
    "        #             l.append(\"c%d\" % (k))\n",
    "        #         else:\n",
    "        #             l.append(\"NOT c%d\" % (k - args.message_size))\n",
    "\n",
    "        print(\" AND \".join(l))\n",
    "\n",
    "print(graphs_train.hypervectors)\n",
    "print(tm.hypervectors)\n",
    "print(graphs_train.edge_type_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load y for test_set and setup Graph Tsetlin machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of sparse structure.\n",
      "Class distribution: Class 1: 4003, Class 2: 3997\n",
      "Dummy accuracy = 0.500375\n"
     ]
    }
   ],
   "source": [
    "from GraphTsetlinMachine.tm import MultiClassGraphTsetlinMachine\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "#TM Hyper paremeters            Include defeault values in NoisyXOR\n",
    "TM_args = {\n",
    "    'number_of_clauses' : 600,           #Default    10\n",
    "    'T' : 300,                          #Default   100\n",
    "    's' : 2.0,                          #Default   1.0\n",
    "    'number_of_state_bits' : 32,        #Default     2\n",
    "    'depth' : 1,                        #Default     8\n",
    "    'message_size' : 256,              #Default   256\n",
    "    'message_bits' : 2,                 #Default     2\n",
    "    'max_included_literals' : 100,       #Default     4\n",
    "    'double_hashing': True              #Default False\n",
    "}\n",
    "\n",
    "epochs_elapsed, train_time_elapsed = 0, 0\n",
    "\n",
    "train_accuracy_across_epochs, test_accuarcy_across_epochs = [], []\n",
    "\n",
    "\n",
    "tm = MultiClassGraphTsetlinMachine(\n",
    "    **TM_args,\n",
    "    grid = (1, 1, 1),\n",
    "    block = (256, 1, 1)\n",
    ")\n",
    "values, counts = np.unique_counts(Y_train)\n",
    "print(f'Class distribution: Class {values[0]}: {counts[0]}, Class {values[1]}: {counts[1]}')\n",
    "print(f'Dummy accuracy = {max(counts)/sum(counts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Accuracy_train: 53.49,  Accuracy_test: 53.25, Accumulated Training Time: 3.36, Testing time: 0.27\n",
      "Epoch: 2, Accuracy_train: 55.33,  Accuracy_test: 54.65, Accumulated Training Time: 6.42, Testing time: 0.28\n",
      "Epoch: 3, Accuracy_train: 65.56,  Accuracy_test: 63.80, Accumulated Training Time: -86.64, Testing time: 0.29\n",
      "Epoch: 4, Accuracy_train: 65.64,  Accuracy_test: 64.85, Accumulated Training Time: -83.65, Testing time: 0.28\n",
      "Epoch: 5, Accuracy_train: 66.67,  Accuracy_test: 65.55, Accumulated Training Time: -80.70, Testing time: 0.28\n",
      "Epoch: 6, Accuracy_train: 71.59,  Accuracy_test: 70.70, Accumulated Training Time: -77.71, Testing time: 0.28\n",
      "Epoch: 7, Accuracy_train: 72.81,  Accuracy_test: 71.75, Accumulated Training Time: -170.84, Testing time: 0.28\n",
      "Epoch: 8, Accuracy_train: 68.55,  Accuracy_test: 67.80, Accumulated Training Time: -168.00, Testing time: 0.32\n",
      "Epoch: 9, Accuracy_train: 76.30,  Accuracy_test: 76.30, Accumulated Training Time: -165.12, Testing time: 0.28\n",
      "Epoch: 10, Accuracy_train: 74.99,  Accuracy_test: 74.45, Accumulated Training Time: -162.22, Testing time: 0.30\n",
      "Epoch: 11, Accuracy_train: 89.26,  Accuracy_test: 89.05, Accumulated Training Time: -159.21, Testing time: -0.11\n",
      "Epoch: 12, Accuracy_train: 91.76,  Accuracy_test: 90.95, Accumulated Training Time: -156.21, Testing time: 0.30\n",
      "Epoch: 13, Accuracy_train: 89.91,  Accuracy_test: 89.00, Accumulated Training Time: -153.48, Testing time: 0.26\n",
      "Epoch: 14, Accuracy_train: 92.12,  Accuracy_test: 91.85, Accumulated Training Time: -150.87, Testing time: 0.26\n",
      "Epoch: 15, Accuracy_train: 93.50,  Accuracy_test: 92.35, Accumulated Training Time: -148.29, Testing time: 0.29\n",
      "Epoch: 16, Accuracy_train: 95.03,  Accuracy_test: 94.10, Accumulated Training Time: -49.63, Testing time: -95.76\n",
      "Epoch: 17, Accuracy_train: 97.88,  Accuracy_test: 98.00, Accumulated Training Time: -47.10, Testing time: 0.27\n",
      "Epoch: 18, Accuracy_train: 98.41,  Accuracy_test: 98.10, Accumulated Training Time: -44.47, Testing time: 0.27\n",
      "Epoch: 19, Accuracy_train: 98.14,  Accuracy_test: 98.05, Accumulated Training Time: -41.99, Testing time: 0.26\n",
      "Epoch: 20, Accuracy_train: 97.58,  Accuracy_test: 97.40, Accumulated Training Time: -39.43, Testing time: 0.27\n",
      "Epoch: 21, Accuracy_train: 93.15,  Accuracy_test: 93.35, Accumulated Training Time: -36.87, Testing time: 0.26\n",
      "Epoch: 22, Accuracy_train: 98.80,  Accuracy_test: 98.65, Accumulated Training Time: -34.45, Testing time: 0.27\n",
      "Epoch: 23, Accuracy_train: 99.06,  Accuracy_test: 98.80, Accumulated Training Time: -32.07, Testing time: 0.27\n",
      "Epoch: 24, Accuracy_train: 98.36,  Accuracy_test: 98.60, Accumulated Training Time: -29.67, Testing time: 0.26\n",
      "Epoch: 25, Accuracy_train: 99.34,  Accuracy_test: 98.70, Accumulated Training Time: -27.23, Testing time: 0.28\n",
      "Epoch: 26, Accuracy_train: 99.28,  Accuracy_test: 99.10, Accumulated Training Time: -24.85, Testing time: 0.27\n",
      "Epoch: 27, Accuracy_train: 99.61,  Accuracy_test: 99.50, Accumulated Training Time: -22.54, Testing time: 0.27\n",
      "Epoch: 28, Accuracy_train: 99.52,  Accuracy_test: 99.35, Accumulated Training Time: -20.24, Testing time: 0.26\n",
      "Epoch: 29, Accuracy_train: 99.50,  Accuracy_test: 99.00, Accumulated Training Time: -18.01, Testing time: 0.27\n",
      "Epoch: 30, Accuracy_train: 99.67,  Accuracy_test: 99.10, Accumulated Training Time: -15.80, Testing time: 0.26\n",
      "Epoch: 31, Accuracy_train: 99.78,  Accuracy_test: 99.65, Accumulated Training Time: -13.51, Testing time: 0.28\n",
      "Epoch: 32, Accuracy_train: 99.79,  Accuracy_test: 99.65, Accumulated Training Time: -11.33, Testing time: 96.29\n",
      "Epoch: 33, Accuracy_train: 99.67,  Accuracy_test: 99.50, Accumulated Training Time: -9.13, Testing time: 0.27\n",
      "Epoch: 34, Accuracy_train: 99.84,  Accuracy_test: 99.70, Accumulated Training Time: -6.93, Testing time: 0.27\n",
      "Epoch: 35, Accuracy_train: 99.85,  Accuracy_test: 99.85, Accumulated Training Time: -4.79, Testing time: 0.26\n",
      "Epoch: 36, Accuracy_train: 99.33,  Accuracy_test: 99.45, Accumulated Training Time: -2.58, Testing time: 0.27\n",
      "Epoch: 37, Accuracy_train: 99.79,  Accuracy_test: 99.75, Accumulated Training Time: -96.41, Testing time: 0.29\n",
      "Epoch: 38, Accuracy_train: 99.91,  Accuracy_test: 99.90, Accumulated Training Time: -94.28, Testing time: 0.27\n",
      "Epoch: 39, Accuracy_train: 99.90,  Accuracy_test: 100.00, Accumulated Training Time: -92.08, Testing time: 0.27\n",
      "100% Accuracy achieved on testset!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert len(graphs_train.number_of_graph_nodes) == len(Y_train)\n",
    "#Train and Eval\n",
    "EPOCHS = 200\n",
    "\n",
    "#print(f'Prior to training: Accuracy_train: {100*(tm.predict(graphs_train) == Y_train).mean()},  Accuracy_test: {100*(tm.predict(graphs_test) == Y_test).mean()}')\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    epochs_elapsed += 1\n",
    "    start_training = time()\n",
    "    tm.fit(graphs_train, Y_train, epochs=1, incremental=True)\n",
    "    stop_training = time()\n",
    "    epoch_train_time = stop_training-start_training\n",
    "    train_time_elapsed += epoch_train_time      \n",
    "\n",
    "\n",
    "    start_testing = time()\n",
    "    result_test = 100*(tm.predict(graphs_test) == Y_test).mean()\n",
    "    stop_testing = time()\n",
    "\n",
    "    result_train = 100*(tm.predict(graphs_train) == Y_train).mean()\n",
    "\n",
    "    train_accuracy_across_epochs.append(result_train)\n",
    "    test_accuarcy_across_epochs.append(result_test)\n",
    "\n",
    "    print(\"Epoch: %d, Accuracy_train: %.2f,  Accuracy_test: %.2f, Accumulated Training Time: %.2f, Testing time: %.2f\" % (epochs_elapsed, result_train, result_test, train_time_elapsed, stop_testing-start_testing))\n",
    "\n",
    "    log_if_best_result(result_test, \n",
    "                       train_accuracy=result_train, \n",
    "                       EPOCHS=epochs_elapsed, \n",
    "                       hypervector_size=HYPERVECTOR_SIZE, \n",
    "                       hypervector_bits=HYPERVECTOR_BITS, \n",
    "                       train_time=train_time_elapsed,\n",
    "                       TM_args=TM_args,\n",
    "                       train_accuracy_across_epochs=train_accuracy_across_epochs,\n",
    "                       test_accuarcy_across_epochs=test_accuarcy_across_epochs)\n",
    "\n",
    "    if result_test == 100:\n",
    "        print('100% Accuracy achieved on testset!')\n",
    "        break\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2x2 Board 100 % Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board Size: 7, Average Clause Length: 99.25166666666667\n",
      "[[ 85  92]\n",
      " [ 76  11]\n",
      " [ 45 108]\n",
      " [ 20   4]\n",
      " [ 11  52]\n",
      " [105  52]\n",
      " [ 12  50]\n",
      " [ 84  56]\n",
      " [ 86  34]\n",
      " [  7  20]\n",
      " [ 31  71]\n",
      " [ 72  22]\n",
      " [ 83  36]\n",
      " [ 69  77]\n",
      " [ 61  57]\n",
      " [ 57  15]\n",
      " [ 86  82]\n",
      " [108  37]\n",
      " [ 85  41]\n",
      " [ 65  45]\n",
      " [ 91  90]\n",
      " [  8 118]\n",
      " [  1  70]]\n",
      "[[  0 507]\n",
      " [  1 506]\n",
      " [  2 505]\n",
      " ...\n",
      " [ 85 412]\n",
      " [ 86 411]\n",
      " [ 87 410]]\n",
      "{'Right': 0, 'UpRight': 1, 'DownRight': 2, 'UpLeft': 3, 'Left': 4, 'DownLeft': 5}\n"
     ]
    }
   ],
   "source": [
    "weights = tm.get_state()[1].reshape(2, -1)\n",
    "clauses = []\n",
    "for i in range(tm.number_of_clauses):\n",
    "        #print(\"Clause #%d W:(%d %d)\" % (i, weights[0,i], weights[1,i]), end=' ')\n",
    "        l = []\n",
    "        for k in range(HYPERVECTOR_SIZE * 2):\n",
    "            if tm.ta_action(0, i, k):\n",
    "                if k < HYPERVECTOR_SIZE:\n",
    "                    l.append(\"x%d\" % (k))\n",
    "                else:\n",
    "                    l.append(\"NOT x%d\" % (k - HYPERVECTOR_SIZE))\n",
    "        clauses.append(l)\n",
    "\n",
    "        \n",
    "        #print(\" AND \".join(l))\n",
    "\n",
    "print(f'Board Size: {board_size}, Average Clause Length: {np.mean([len(clause) for clause in clauses])}')\n",
    "\n",
    "print(graphs_train.hypervectors)\n",
    "print(tm.hypervectors)\n",
    "print(graphs_train.edge_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Interpretability Analysis Function\n",
    "def interpretability_analysis(tm, all_node_names, board_size):\n",
    "    # Get clauses and state weights\n",
    "    weights = tm.get_state()[1].reshape(2, -1)\n",
    "    clauses = []\n",
    "    for i in range(tm.number_of_clauses):\n",
    "        clause_literals = []\n",
    "        for k in range(HYPERVECTOR_SIZE * 2):  # Adjust to match your graph hypervector size\n",
    "            if tm.ta_action(0, i, k):\n",
    "                if k < HYPERVECTOR_SIZE:\n",
    "                    clause_literals.append(f\"x{k}\")\n",
    "                else:\n",
    "                    clause_literals.append(f\"NOT x{k - HYPERVECTOR_SIZE}\")\n",
    "        clauses.append(clause_literals)\n",
    "\n",
    "    # Frequent Itemset Mining\n",
    "    print(\"\\n### Frequent Itemset Mining ###\")\n",
    "    \n",
    "    # Convert clauses into a transaction-style binary DataFrame\n",
    "    all_literals = sorted({literal for clause in clauses for literal in clause})\n",
    "    binary_transactions = []\n",
    "    for clause in clauses:\n",
    "        binary_row = {literal: (literal in clause) for literal in all_literals}\n",
    "        binary_transactions.append(binary_row)\n",
    "    \n",
    "    transactions_df = pd.DataFrame(binary_transactions)\n",
    "    \n",
    "    # Ensure no NaN values are present\n",
    "    transactions_df = transactions_df.fillna(False)\n",
    "    \n",
    "    # Apply Apriori\n",
    "    frequent_itemsets = apriori(transactions_df, min_support=0.1, use_colnames=True)\n",
    "    print(frequent_itemsets)\n",
    "    \n",
    "\n",
    "    # Feature Importance\n",
    "    print(\"\\n### Feature Importance ###\")\n",
    "    feature_count = {}\n",
    "    for clause in clauses:\n",
    "        for literal in clause:\n",
    "            feature_count[literal] = feature_count.get(literal, 0) + 1\n",
    "    feature_importance = sorted(feature_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    for literal, count in feature_importance:\n",
    "        print(f\"{literal}: {count}\")\n",
    "\n",
    "    # Importance Matrix\n",
    "    print(\"\\n### Importance Matrix ###\")\n",
    "    importance_matrix = np.zeros((board_size, board_size))\n",
    "    for clause in clauses:\n",
    "        for literal in clause:\n",
    "            if literal.startswith(\"x\"):  # Adjust to your literal naming convention\n",
    "                idx = int(literal[1:])\n",
    "                row, col = divmod(idx, board_size)\n",
    "                importance_matrix[row, col] += 1\n",
    "\n",
    "    sns.heatmap(importance_matrix, annot=True, fmt=\".2f\")\n",
    "    plt.title(\"Feature Importance Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "    # Clause Complexity\n",
    "    print(\"\\n### Clause Complexity ###\")\n",
    "    for idx, clause in enumerate(clauses):\n",
    "        complexity = len(clause)\n",
    "        print(f\"Clause #{idx}: Complexity = {complexity}\")\n",
    "    \n",
    "    return importance_matrix\n",
    "\n",
    "\n",
    "# Run Interpretability Analysis after training\n",
    "#importance_matrix = interpretability_analysis(tm, all_node_names, board_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_hex_board_importance(importance_matrix, board_size):\n",
    "    \"\"\"\n",
    "    Plots a hexagonal board with cell importance visualized as colors.\n",
    "    \"\"\"\n",
    "    # Create a hexagonal grid\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Normalize importance values for color mapping\n",
    "    normalized_importance = importance_matrix / np.max(importance_matrix)\n",
    "    \n",
    "    # Plot hexagons for each cell\n",
    "    for row in range(board_size):\n",
    "        for col in range(board_size):\n",
    "            # Shift alternate rows for hex grid alignment\n",
    "            x = col + (0.5 * (row % 2))\n",
    "            y = -row\n",
    "            hex_color = plt.cm.viridis(normalized_importance[row, col])  # Use a colormap for importance\n",
    "            hexagon = plt.Polygon(\n",
    "                [\n",
    "                    (x + np.cos(angle), y + np.sin(angle))\n",
    "                    for angle in np.linspace(0, 2 * np.pi, 7)\n",
    "                ],\n",
    "                edgecolor=\"black\",\n",
    "                facecolor=hex_color,\n",
    "            )\n",
    "            ax.add_patch(hexagon)\n",
    "            # Annotate the importance value\n",
    "            ax.text(x, y, f\"{importance_matrix[row, col]:.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "    # Set plot limits and labels\n",
    "    ax.set_xlim(-1, board_size + 1)\n",
    "    ax.set_ylim(-board_size, 1)\n",
    "    ax.axis(\"off\")\n",
    "    plt.title(\"Hex Board Cell Importance\")\n",
    "    plt.show()\n",
    "\n",
    "# Generate importance matrix for testing (replace with actual importance matrix)\n",
    "# This should come from the `interpretability_analysis` function\n",
    "#importance_matrix = np.random.rand(board_size, board_size)  # Replace with your actual matrix\n",
    "\n",
    "# Call the function to plot the board\n",
    "#plot_hex_board_importance(importance_matrix, board_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
